# TeamAlpha Interactive Client - Complete Setup

## âœ… What's Ready

### 1. Running Services
```
dev_ollama           â†’ localhost:11435 (LLM inference)
dev_teamalpha_agent  â†’ localhost:18080 (Agent API)
```

### 2. Interactive Client Files
- **`interactive_client.py`** - Main CLI for agent interaction
- **`run-client.sh`** - Bash launcher script
- **`setup_and_run.py`** - Python setup with model waiting

### 3. Agent Team (5 Members)
- **Alice** (Engineer) - Technical implementation
- **Bob** (Code Reviewer) - Quality assurance & reviews
- **Eve** (Architect) - System design & patterns  
- **Charlie** (QA Engineer) - Testing & validation
- **Diana** (Product Manager) - Strategy & requirements

---

## ğŸš€ Launch Options

### **Option 1: Quick Start (Recommended)**
```bash
./run-client.sh
# or
.venv/bin/python3 interactive_client.py
```

### **Option 2: With Model Status Checker**
```bash
.venv/bin/python3 setup_and_run.py
```

---

## ğŸ“ Commands Inside Client

```
status              â†’ Check agent health âœ…
team create         â†’ Create 5-agent team ğŸ‘¥
agents              â†’ List team members ğŸ“‹
generate <prompt>   â†’ Send to LLM ğŸ¤–
assign <name> <task>â†’ Assign to specific agent ğŸ¯
help                â†’ Show all commands ğŸ“š
exit                â†’ Quit client ğŸ‘‹
```

---

## ğŸ’¡ Usage Examples

### Create Team & Check Status
```bash
teamalpha> team create
teamalpha> agents
teamalpha> status
```

### Send Task to Specific Agent
```bash
teamalpha> assign Alice "Create a Python FastAPI endpoint for user login"
teamalpha> assign Eve "Design the database schema"
teamalpha> assign Bob "Review the implementation"
```

### Query LLM Directly
```bash
teamalpha> generate "Explain how REST APIs work"
teamalpha> generate "Write a Python decorator for authentication"
```

---

## â³ Model Status

**llama3 is downloading** (~4.7GB, ~2-5 minutes on typical connection)

### Monitor Download
```bash
# Check if model is ready
docker exec dev_ollama ollama list

# View download progress
docker logs dev_ollama | tail -20

# Restart if stuck
docker restart dev_ollama
```

### While Waiting
âœ… You can still run these commands:
- `status` - Check agent health
- `team create` - Set up team
- `agents` - List agents
- `help` - Show help

âŒ These will fail until model loads:
- `generate <prompt>` - Requires LLM
- `assign <name> <task>` - Requires LLM

---

## ğŸ³ Container Commands

### Start Stack
```bash
docker compose -f infrastructure/docker-compose.dev.yml up -d
```

### Stop Stack
```bash
docker compose -f infrastructure/docker-compose.dev.yml down
```

### View Logs
```bash
docker logs -f dev_teamalpha_agent    # Agent server
docker logs -f dev_ollama             # LLM service
```

### Force Model Download
```bash
docker exec dev_ollama ollama pull llama3
```

---

## ğŸ” Troubleshooting

### Client Won't Connect
```bash
# Verify containers running
docker ps | grep dev_

# Restart stack
docker compose -f infrastructure/docker-compose.dev.yml restart

# Check agent logs
docker logs dev_teamalpha_agent
```

### LLM Errors After Generate
```bash
# Model still downloading? Check:
docker exec dev_ollama ollama list

# Manual pull if needed:
docker exec dev_ollama ollama pull llama3

# Check model loaded:
curl http://localhost:11435/api/tags
```

### Import/Dependency Errors
```bash
# Reinstall Python dependencies
uv sync

# Verify venv
ls -la .venv/bin/python3
```

---

## ğŸ“š File Structure

```
interactive_client.py    â† Main client (START HERE)
run-client.sh           â† Bash launcher
setup_and_run.py        â† Python launcher with model wait
INTERACTIVE_CLIENT.md   â† Full documentation
CLIENT_SETUP.md         â† This file

src/teamalpha/
  â”œâ”€â”€ agent.py          â† Agent class & LLM integration
  â”œâ”€â”€ team.py           â† Team orchestration
  â””â”€â”€ client.py         â† HTTP client library

server.py               â† FastAPI wrapper (running)
```

---

## ğŸ¯ Next Steps

1. **Start Client** â†’ `./run-client.sh`
2. **Create Team** â†’ `team create`
3. **Check Status** â†’ `status`
4. **Try Assignment** â†’ `assign Alice "Design an API"`
5. **Monitor Logs** â†’ `docker logs -f dev_ollama`
6. **Query LLM** â†’ `generate "Your prompt"`

---

## ğŸ“Š Current State

| Component | Status | Port | Notes |
|-----------|--------|------|-------|
| Ollama | âœ… Running | 11435 | Model downloading |
| Agent API | âœ… Running | 18080 | Health: OK |
| Client | âœ… Ready | N/A | Use ./run-client.sh |
| Team | âœ… Ready | N/A | 5 agents configured |

---

## ğŸ”— Related Files

- **Full Documentation**: `INTERACTIVE_CLIENT.md`
- **Framework Guide**: `docs/TEAM_GUIDE.md`
- **Stack Setup**: `docs/DEV_RUN.md`
- **Project Structure**: `STRUCTURE_GUIDE.md`

---

**Ready to go! Launch with:** `./run-client.sh`
