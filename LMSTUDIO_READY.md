# ğŸ‰ TeamAlpha + LM Studio - Integration Complete!

You can now connect your agent fleet directly to LM Studio's **gpt-oss-20b-GGUF** model running locally on Windows.

## âš¡ 30-Second Start

```bash
# 1. Start LM Studio on Windows (ensure gpt-oss-20b-GGUF is loaded)

# 2. Run setup (from project directory)
python3 setup_lmstudio.py

# 3. In the interactive client:
> team create
> assign Alice "Design a REST API"
> exit
```

---

## ğŸ“¦ What's New

### 6 New Files Created

| File | Purpose |
|------|---------|
| `src/teamalpha/lmstudio.py` | LM Studio HTTP client (OpenAI-compatible) |
| `src/teamalpha/llm_config.py` | Configuration management |
| `test_lmstudio.py` | Connection & functionality tests |
| `setup_lmstudio.py` | One-command setup + auto-launch |
| `LMSTUDIO_SETUP.md` | Configuration reference |
| `LMSTUDIO_INTEGRATION.md` | Complete guide (8.4KB) |

### 2 Files Modified

| File | Changes |
|------|---------|
| `src/teamalpha/agent.py` | Added LM Studio provider support + auto-detection |
| `interactive_client.py` | Environment variable support for provider selection |

---

## ğŸ”Œ Three Ways to Connect

### 1. Easiest: One-Command Setup â­
```bash
python3 setup_lmstudio.py
```
Handles everything automatically.

### 2. Environment Variables
```bash
$env:LLM_PROVIDER = "lmstudio"
python3 interactive_client.py
```

### 3. Direct Python
```python
from src.teamalpha.agent import Agent, AgentRole

agent = Agent(
    name="Alice",
    role=AgentRole.ENGINEER,
    provider="lmstudio"
)
```

---

## âœ… Your Agent Fleet Works With:

- âœ… **gpt-oss-20b-GGUF** (your local model)
- âœ… **All 5 agent roles** (Engineer, Reviewer, Architect, QA, PM)
- âœ… **Interactive CLI** (talk to agents in real-time)
- âœ… **Team workflows** (PM â†’ Architect â†’ Engineer â†’ QA)
- âœ… **Fallback to Ollama** (if LM Studio not available)
- âœ… **Auto-detection** (tries best available LLM)

---

## ğŸ§ª Test It

```bash
# Quick test
python3 test_lmstudio.py

# Full diagnostics
python3 test_lmstudio.py --all

# Test generation
python3 test_lmstudio.py --test-generation

# Test with agent
python3 test_lmstudio.py --test-agent
```

---

## ğŸ“‹ Configuration Reference

### LM Studio Default Settings
```
Host: http://localhost:1234
Model: gpt-oss-20b-GGUF
Temperature: 0.7
Max Tokens: 500
Timeout: 120 seconds
```

### Environment Variables
```bash
LLM_PROVIDER=lmstudio      # Use LM Studio
LLM_PROVIDER=ollama        # Use Ollama
LLM_PROVIDER=auto          # Auto-detect (LM Studio â†’ Ollama)
LMSTUDIO_HOST=...          # Custom LM Studio URL
```

---

## ğŸ“š Documentation

| File | Content |
|------|---------|
| `LMSTUDIO_INTEGRATION.md` | **Start here** - Complete guide with examples |
| `LMSTUDIO_SETUP.md` | Configuration and troubleshooting |
| `test_lmstudio.py` | Test your connection |
| `setup_lmstudio.py` | Quick setup script |

---

## ğŸš€ Usage Examples

### Example 1: Interactive Team
```bash
python3 setup_lmstudio.py

teamalpha> team create
teamalpha> assign Alice "Build a login system"
teamalpha> assign Bob "Review the code"
teamalpha> exit
```

### Example 2: Get Different Perspectives
```bash
teamalpha> team create
teamalpha> assign Alice "Design a database schema"
teamalpha> assign Eve "Review the design"
teamalpha> assign Charlie "Plan test cases"
teamalpha> exit
```

### Example 3: Direct Agent
```bash
LLM_PROVIDER=lmstudio python3 -c "
from src.teamalpha.agent import Agent, AgentRole
alice = Agent('Alice', AgentRole.ENGINEER, provider='lmstudio')
print(alice.think('Explain REST APIs'))
"
```

---

## ğŸ”„ How It Works

```
LM Studio (Windows)          Your Project
    â†“                              â†“
gpt-oss-20b-GGUF        src/teamalpha/lmstudio.py
    â†“                              â†“
Port: 1234      â†HTTP/RESTâ†’    OpenAI-compatible
    â†“                            client
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Agent framework
                                    â†“
                            5 specialized agents
                                    â†“
                            Interactive CLI
```

---

## ğŸ¯ Key Features

âœ… **Fully Local** - No cloud API calls, everything runs on your machine
âœ… **OpenAI Compatible** - Uses standard OpenAI API format
âœ… **Auto-Detection** - Automatically finds LM Studio or Ollama
âœ… **Fallback Logic** - Seamlessly switches between LLM providers
âœ… **Specialized Agents** - 5 roles with unique perspectives
âœ… **Real-Time Chat** - Interactive CLI interface
âœ… **Team Workflows** - Coordinate multi-agent tasks
âœ… **Backward Compatible** - Still works with Ollama

---

## ğŸ”§ Troubleshooting

### LM Studio Not Detected
```bash
# Verify LM Studio is running
curl http://localhost:1234/health

# Check loaded models
curl http://localhost:1234/v1/models

# Run full test
python3 test_lmstudio.py --all
```

### Wrong Model Name
```bash
# List available models
python3 test_lmstudio.py

# Load gpt-oss-20b-GGUF in LM Studio UI
# Then retry
```

### Connection Issues
1. Ensure LM Studio is running on Windows
2. Verify model is loaded
3. Check port 1234 is accessible
4. Run: `python3 test_lmstudio.py`

---

## ğŸ’¡ Next Steps

1. **Start LM Studio** on Windows
2. **Load model**: gpt-oss-20b-GGUF
3. **Run setup**: `python3 setup_lmstudio.py`
4. **Create team**: `team create`
5. **Assign tasks**: `assign Alice "Your task"`

---

## ğŸ“Š What's Included

### Agent Framework
- 5 specialized agent roles
- Multi-agent coordination
- Message passing system
- Tool integration support

### LM Studio Integration
- OpenAI-compatible HTTP client
- Health checks
- Model listing
- Error handling

### Interactive Client
- Real-time command loop
- Team management
- Task assignment
- Help system

### Testing & Setup
- Connection tests
- Generation tests
- Full system diagnostics
- One-command setup

---

## ğŸ“ Learning Resources

**Beginner** (5 min): `python3 setup_lmstudio.py`

**Intermediate** (15 min): Read `LMSTUDIO_INTEGRATION.md`

**Advanced** (30 min): Study `src/teamalpha/agent.py` and `lmstudio.py`

---

## âœ¨ You're All Set!

Your agent fleet is ready to work with LM Studio.

**Start with:**
```bash
python3 setup_lmstudio.py
```

**Then use your local AI team!** ğŸš€

---

*For questions or issues, check LMSTUDIO_INTEGRATION.md or LMSTUDIO_SETUP.md*
