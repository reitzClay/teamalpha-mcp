# TeamAlpha: MCP-Enabled AI Agent System

> **Your AI agent now has superpowers!** ğŸ¤–âš¡

An intelligent agent system that combines:
- **LLM** (Language Model via Ollama)
- **MCP** (Model Context Protocol for tools)
- **Docker** (containerized deployment)

## What Can It Do?

âœ¨ **Search GitHub** â€” Find repositories, read READMEs, get stats
ğŸ“‚ **Access Files** â€” Read and analyze local files safely
ğŸ§  **AI Analysis** â€” Synthesize information intelligently
ğŸ”„ **Multi-Step Workflows** â€” Complex tasks with tool chains

## Quick Start

```bash
# See it in action (3-4 minutes)
docker compose exec teamalpha-agent python mcp_tool_demo.py

# Watch realistic workflow (3-4 minutes)
docker compose exec teamalpha-agent python mcp_workflow_example.py
```

## Documentation Index

| Document | Purpose | Time |
|----------|---------|------|
| **INDEX.md** | Navigation hub | 5 min |
| **MCP_SUMMARY.md** | What you have | 10 min |
| **MCP_INTEGRATION_GUIDE.md** | How it works | 30 min |
| **MCP_QUICK_REFERENCE.md** | Quick lookup | As needed |
| **MCP_ARCHITECTURE.md** | System design | 15 min |

## Demo Scripts

```bash
# 1. Basic tool execution (START HERE)
docker compose exec teamalpha-agent python mcp_tool_demo.py

# 2. Realistic agent workflow
docker compose exec teamalpha-agent python mcp_workflow_example.py

# 3. Tool-aware agent
docker compose exec teamalpha-agent python mcp_langchain_agent.py

# 4. Advanced execution patterns
docker compose exec teamalpha-agent python mcp_executor_agent.py
```

## Available Tools

### ğŸ“‚ Filesystem
- `read_file(path)` â€” Read file contents
- `list_directory(path)` â€” List directory contents

### ğŸ™ GitHub
- `search_repositories(query)` â€” Search for repos
- `get_repository_readme(owner, repo)` â€” Fetch README
- `get_repo_stats(owner, repo)` â€” Get repo statistics

### ğŸ§  LLM Analysis
- Ollama llama3 model (local)
- Real-time inference
- Context-aware responses

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Compose                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Ollama (LLM)  â†â†’  Python Agent        â”‚
â”‚  (llama3)           (LangChain)        â”‚
â”‚  11434              + MCP Tools        â”‚
â”‚                                         â”‚
â”‚                 â†“â†“â†“                     â”‚
â”‚                                         â”‚
â”‚          MCP Tool Layer                â”‚
â”‚    â”œâ”€ GitHub API access               â”‚
â”‚    â”œâ”€ Filesystem access               â”‚
â”‚    â””â”€ Extensible framework            â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Concepts

**MCP** = "USB-C for AI"
- Standard interface for tools
- Safe access to external systems
- LLM can request tool use

**Tool Chain** = Multiple tools working together
```
Search GitHub
    â†“
Fetch README
    â†“
Analyze with LLM
    â†“
Generate response
```

## Tech Stack

- **Language:** Python 3.12
- **LLM:** Ollama (llama3 model)
- **Framework:** LangChain
- **Container:** Docker Compose
- **APIs:** GitHub REST API, local filesystem

## Docker Commands

```bash
# Start everything
docker compose up -d

# Run a demo
docker compose exec teamalpha-agent python mcp_tool_demo.py

# View logs
docker compose logs -f

# Stop everything
docker compose down

# Rebuild image
docker compose up -d --build
```

## Use Cases

### Research Assistant
```
"Find and analyze crewAI on GitHub"
â†’ Searches GitHub
â†’ Fetches README
â†’ Gets statistics
â†’ Analyzes with AI
```

### Code Review
```
"Analyze our project structure"
â†’ Reads local files
â†’ Checks pyproject.toml
â†’ LLM provides insights
```

### Project Comparison
```
"Compare 3 Python frameworks"
â†’ Searches each on GitHub
â†’ Fetches READMEs
â†’ Collects statistics
â†’ LLM synthesizes comparison
```

## Features

âœ… **Real-time Data Access** â€” Not limited to training data
âœ… **Safe Tool Execution** â€” Controlled access patterns
âœ… **Intelligent Synthesis** â€” AI analyzes tool results
âœ… **Extensible Architecture** â€” Easy to add new tools
âœ… **Containerized** â€” Deploy anywhere
âœ… **Production-Ready** â€” Error handling, logging
âœ… **Well-Documented** â€” 5 detailed guides + examples

## Performance

- **LLM Response:** 1-3 seconds (llama3 on RTX 4070)
- **GitHub Search:** <1 second per query
- **File Reading:** <100ms for typical files
- **Overall Workflow:** 3-5 seconds for complex tasks

## Next Steps

1. **Run a demo** â†’ See tools in action
2. **Read documentation** â†’ Understand MCP
3. **Explore scripts** â†’ Learn patterns
4. **Customize tools** â†’ Add your own
5. **Deploy** â†’ Push to production

## Resources

- **MCP Official:** https://modelcontextprotocol.io
- **Tool Registry:** https://smithery.ai
- **GitHub:** https://github.com/modelcontextprotocol

## Summary

You have a **fully functional AI agent** that can:
- ğŸ” Research in real-time
- ğŸ“Š Analyze data intelligently
- ğŸ”— Connect multiple tools
- ğŸ“ˆ Scale to production
- ğŸ› ï¸ Extend with new capabilities

**Start exploring:** `docker compose exec teamalpha-agent python mcp_tool_demo.py`

---

**Built with:** LangChain + Ollama + MCP + Docker
**Status:** Production-Ready âœ…
**Maintenance:** Actively updated
