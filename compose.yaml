version: '3.8'

services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

  teamalpha-agent:
    build: .
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    ports:
      - "8080:8080"
    command: ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8080"]

volumes:
  ollama_data: {}
version: '3.8'

services:
  # The LLM Power Plant. This is the only service our agent depends on.
  ollama:
    image: ollama/ollama
    deploy:
      version: "3.8"

      services:
        ollama:
          image: ollama/ollama
          deploy:
            resources:
              reservations:
                devices:
                  - driver: nvidia
                    count: 1
                    capabilities: [gpu]
          ports:
            - "11434:11434"
          volumes:
            - ollama_data:/root/.ollama

        teamalpha-agent:
          build: .
          environment:
            - OPENAI_API_KEY=sk-dummy-key-for-local-ollama
            - OLLAMA_HOST=http://ollama:11434
          depends_on:
            - ollama
          ports:
            - "8080:8080"
          command: ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8080"]

      volumes:
        ollama_data: {}
    deploy:
